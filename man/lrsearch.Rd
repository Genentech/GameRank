% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lrsearch.R
\name{lrsearch}
\alias{lrsearch}
\alias{lrsearch.formula}
\title{Plus-L, Minus-R search algorithm}
\usage{
lrsearch(
  dat,
  resp,
  vars,
  fn_train = fn_train_binomial,
  fn_eval = fn_eval_binomial,
  m = NULL,
  ds = 5L,
  maximize = TRUE,
  L,
  R,
  kmax = 1000,
  ...
)

lrsearch.formula(
  fo,
  dat,
  fn_train = fn_train_binomial,
  fn_eval = fn_eval_binomial,
  m = NULL,
  ds = 5L,
  maximize = TRUE,
  L,
  R,
  kmax = 1000,
  ...
)
}
\arguments{
\item{dat}{data.frame or tibble comprising data for model generation and validation.}

\item{resp}{Character string defining the response (lhs) of the model formula.}

\item{vars}{Character vector defining the list of variables for selection. Those are concatenated by '+' 
as the right hand side (rhs) of the modelling formula.}

\item{fn_train}{Function with signature function( dat, resp, selection, ... ) that returns a model or NULL in any other case on the given data dat.}

\item{fn_eval}{Function with signature function( dat, resp, selection, ... ) that returns a real number or NA in any other case, e.g. when model is NULL.}

\item{m}{Size of final partition size. 
Note this parameter is used for stopping only. Best selection will be determined by whole set of evaluated selections, i.e., can be larger than m.}

\item{ds}{Definition of (parallel) training:validation splits
- a matrix with d columns containing 1s and 2s, where 1 denotes sample is used for training the model and 2 denotes sample used for validation.
  The average of all d training:validation results is used for selection.
- an integer number determing the number of random training:validation splits that should be generated. The sampling will ensure a sufficient number
  of complete cases in the training split.}

\item{maximize}{A logic value determining if fn_eval is maximized (set to TRUE) or minimized (set to FALSE).}

\item{L}{Number of forward steps per iteration.}

\item{R}{Number of backward steps per iteration.}

\item{kmax}{Limit number of iterations}

\item{...}{An other arguments passed to fn_train or fn_eval during calls, e.g. maybe 'u = 365' for Survival evaluations specifying the landmark day.}

\item{fo}{Only for call with formula as first argument. Extracts lhs ~ rhs into resp and vars, and calls backward( dat, resp, vars, ... )}
}
\value{
List with elements
\describe{
 \item{response}{As from input parameters}
 \item{variables}{As from input parameters}
 \item{m}{As from input parameters}
 \item{splits}{As from input parameters}
 \item{maximize}{As from input parameters}
 \item{L}{As from input parameters}
 \item{R}{As from input parameters}
 \item{kmax}{As from input parameters}
 \item{start}{Start time of core algorithm loop}
 \item{end}{End time of core algorithm loop}
 \item{variable_selections}{Best selections overall (regardless of m)}
 \item{results}{Dataset with one record per train:validation evaluation}
 \item{agg_results}{Dataset with averaged performance over splits}
}
}
\description{
Performs L forward and R backward selection steps per iteration until a
desired partition size is reached. Depending on L<R or L>R it starts with either the
full set of variables or the empty set of variables.
}
\details{
The Plus-L, Minus-R algorithm runs as follows:
\code{ \cr
1. if L > R then  \cr
     Y0 = {} \cr
   else  \cr
     Y0 = X; go to step 3 \cr
   fi \cr
2. repeat L times \cr
     x+ = arg max_{x not in Yk} J( Yk + x ) \cr
     Yk+1 = Yk + x+; k = k + 1 \cr
3. repeat R times \cr
     x- = arg max_{x in Yk} J( Yk - x ) \cr
     Yk+1 = Yk - x-; k = k + 1 \cr
4. go to step 2 \cr
}
}
