% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_small_sample.R
\name{model_small_sample}
\alias{model_small_sample}
\alias{fn_train_dummy}
\alias{ff_fn_eval_cross_validation}
\alias{ff_fn_eval_bootstrap}
\title{Small Sample Variable Selection helper functions}
\usage{
fn_train_dummy(dat, resp, selection, ...)

ff_fn_eval_cross_validation(
  arg_fn_train = NULL,
  arg_fn_eval = NULL,
  arg_n_folds = 10L
)

ff_fn_eval_bootstrap(
  arg_fn_train = NULL,
  arg_fn_eval = NULL,
  arg_n_boots = 25L
)
}
\arguments{
\item{dat}{data.frame or tibble rows from the full dataseet provided to the
wrapper that should be used for generating or evaluating models.}

\item{resp}{Response variable being the lhs of the model formula}

\item{selection}{Current selection for model generation or evaluation}

\item{...}{Any other arguments passed to both types of functions,}

\item{arg_fn_train}{Function to generate model on non-hold-out 
folds (fn_train)}

\item{arg_fn_eval}{Function for validation on hold-out fold (fn_eval)}

\item{arg_n_folds}{Number of cross-validation folds to use}

\item{arg_n_boots}{Number of bootstrapped folds to use}

\item{mod}{For evaluation functions the model to be evaluated on dat}
}
\value{
fn_train_... functions return a fitted model object or NULL if that
fails. fn_eval_... functions return a numeric real value measuring the 
validation performance on the given data, or NA if that fails.
}
\description{
Wrapper-based variable selection makes use of two functions, one
for model training and one for model evaluation. This roots in the assumption
of using the split set method for measuring generalization performance on 
true hold-out data. However, when you have a very small sample set this might 
seem problematic.

An way out then could be to estimate generalization performance using 
**cross-validation** or the **bootstrap**. In both approaches, the training 
function must be called on repeated subsamples (folds) such that the error
can be estimated by combining predictions from the excluded samples across 
all folds.

Here we'll provide two function factory methods, ie functions that take a 
training or a validation function and wrap it into one of these algorithms 
such that the call to the wrapper functions keeps the same interface. 
The idea will be to replace fn_train by a dummy method that just returns a 
dummy model. The fn_eval parameter will then consist of a function that
incorporates the right fn_train function and applies it within the estimation 
procedure.

\describe{
\item{fn_train_dummy}{Dummy train function that just returns and 
class "DummyModel" object.}
\item{ff_fn_eval_cross_validation}{Function generator (ff_) function that 
takes an fn_train and fn_eval function together with n_folds parameter, and 
returns a function that calls fn_train and fn_eval to return a n_fold
cross-validated estimate.}
}
}
\examples{
vars <- grep( "the_|rnd", colnames(toy_data), value=TRUE )
resp <- "resp"
ipos <- which(  toy_data$resp )
ineg <- which( !toy_data$resp )
smtoy <- rbind( 
  # draw 50 random observations from each response category
  toy_data[ ipos[1:25],], 
  toy_data[ ineg[1:25],]
)
smtoy

# ff_fn_eval_bootstrap and ff_fn_eval_cross_validation are function factories
# that return a created function which makes use of the supplied fn_train and
# fn_eval function during bootstrap or cross-validation.
fn_bt_eval_binomial_auroc <- ff_fn_eval_bootstrap( fn_train_binomial, 
                                                   fn_eval_binomial_auroc, 
                                                   25L )

# For small sample sizes, the GameRank index vector dsi needs to be set 
# to c(2L,2L) such that all observations are used for validation. Similarly, 
# the training:validation split matrix then also completely consists of 2s.
res <- game_rank( smtoy, resp, vars, 
                  fn_train_dummy, fn_bt_eval_binomial_auroc, 
                  4L, c(2L,2L), TRUE, 3L, 5L, 3L )
res$game_rank_selection
res$variable_ranking


}
