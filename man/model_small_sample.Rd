% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_small_sample.R
\name{model_small_sample}
\alias{model_small_sample}
\alias{fn_train_dummy}
\alias{ff_fn_eval_cross_validation}
\alias{ff_fn_eval_bootstrap}
\title{Small Sample Variable Selection helper functions}
\usage{
fn_train_dummy(dat, resp, selection, ...)

ff_fn_eval_cross_validation(
  arg_fn_train = NULL,
  arg_fn_eval = NULL,
  arg_n_folds = 10L
)

ff_fn_eval_bootstrap(
  arg_fn_train = NULL,
  arg_fn_eval = NULL,
  arg_n_boots = 25L
)
}
\description{
Wrapper-based variable selection makes use of two functions, one
for model training and one for model evaluation. This roots in the assumption of
using the split set method for measuring generalization performance on true hold-out
data. However, when you have a very small sample set this might seem problematic.

An way out then could be to estimate generalization performance using **cross-validation**
or the **bootstrap**. In both approaches, the training function must be called on
repeated subsamples (folds) such that the error can be estimated by combining 
predictions from the excluded samples across all folds.

Here we'll provide two function factory methods, ie functions that take a training
or a validation function and wrap it into one of these algorithms such that the
call to the wrapper functions keeps the same interface. 
The idea will be to replace fn_train by a dummy method that just returns a 
dummy model. The fn_eval parameter will then consist of a function that incorporates
the right fn_train function and applies it within the estimation proceedure.

\describe{
\item{fn_train_dummy}{Dummy train function that just returns and class "DummyModel" object.}
\item{ff_fn_eval_cross_validation}{Function generator (ff_) function that takes an fn_train and fn_eval function 
together with n_folds parameter, and returns a function that calls fn_train and fn_eval to return a n_fold
cross-validated estimate.}
}
}
