@article{huang,
  author  = {Tzu-Kuo Huang and Chih-Jen Lin and Ruby C. Weng},
  title   = {Ranking Individuals by Group Comparisons},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {72},
  pages   = {2187-2216},
  url     = {http://jmlr.org/papers/v9/huang08a.html}
}

@article{trial,
  author = {Will Harris and Joeseph N. Paulson},
  title  = {TRAIL score: A simple model to predict immunochemotherapy tolerability in patients with diffuse large B-cell lymphoma},
  journal = {Clinical Oncology},
  year = {2021}
}

@article{ici,
author = {Austin, Peter C. and Harrell Jr, Frank E. and van Klaveren, David},
title = {Graphical calibration curves and the integrated calibration index (ICI) for survival models},
journal = {Statistics in Medicine},
volume = {39},
number = {21},
pages = {2714-2742},
keywords = {calibration, model validation, random forests, survival analysis, time-to-event model},
doi = {https://doi.org/10.1002/sim.8570},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8570},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8570},
abstract = {Abstract In the context of survival analysis, calibration refers to the agreement between predicted probabilities and observed event rates or frequencies of the outcome within a given duration of time. We aimed to describe and evaluate methods for graphically assessing the calibration of survival models. We focus on hazard regression models and restricted cubic splines in conjunction with a Cox proportional hazards model. We also describe modifications of the Integrated Calibration Index, of E50 and of E90. In this context, this is the average (respectively, median or 90th percentile) absolute difference between predicted survival probabilities and smoothed survival frequencies. We conducted a series of Monte Carlo simulations to evaluate the performance of these calibration measures when the underlying model has been correctly specified and under different types of model mis-specification. We illustrate the utility of calibration curves and the three calibration metrics by using them to compare the calibration of a Cox proportional hazards regression model with that of a random survival forest for predicting mortality in patients hospitalized with heart failure. Under a correctly specified regression model, differences between the two methods for constructing calibration curves were minimal, although the performance of the method based on restricted cubic splines tended to be slightly better. In contrast, under a mis-specified model, the smoothed calibration curved constructed using hazard regression tended to be closer to the true calibration curve. The use of calibration curves and of these numeric calibration metrics permits for a comprehensive comparison of the calibration of competing survival models.},
year = {2020}
}


@article{crowson,
author = {Cynthia S Crowson and Elizabeth J Atkinson and Terry M Therneau},
title ={Assessing calibration of prognostic risk scores},
journal = {Statistical Methods in Medical Research},
volume = {25},
number = {4},
pages = {1692-1706},
year = {2016},
doi = {10.1177/0962280213497434},
    note ={PMID: 23907781},
URL = {https://doi.org/10.1177/0962280213497434},
eprint = {https://doi.org/10.1177/0962280213497434},
    abstract = { Current methods used to assess calibration are limited, particularly in the assessment of prognostic models. Methods for testing and visualizing calibration (e.g. the Hosmer–Lemeshow test and calibration slope) have been well thought out in the binary regression setting. However, extension of these methods to Cox models is less well known and could be improved. We describe a model-based framework for the assessment of calibration in the binary setting that provides natural extensions to the survival data setting. We show that Poisson regression models can be used to easily assess calibration in prognostic models. In addition, we show that a calibration test suggested for use in survival data has poor performance. Finally, we apply these methods to the problem of external validation of a risk score developed for the general population when assessed in a special patient population (i.e. patients with particular comorbidities, such as rheumatoid arthritis). }
}



@article{calster,
  author = {Van Calster, B. and McLernon, D.J. and van Smeden, M.},
  title  = {Calibration: the Achilles heel of predictive analytics},
  journal = {BMC Medicine},
  year = {2019},
  volume = {17},
  issue = {230},
  doi = {https://doi.org/10.1186/s12916-019-1466-7}
}


@article{walsh,
title = {Beyond discrimination: A comparison of calibration methods and clinical usefulness of predictive models of readmission risk},
journal = {Journal of Biomedical Informatics},
volume = {76},
pages = {9-18},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417302277},
author = {Colin G. Walsh and Kavya Sharman and George Hripcsak},
keywords = {Readmissions, Predictive analytics, Calibration, Utility analysis, Clinical usefulness},
abstract = {Background
Prior to implementing predictive models in novel settings, analyses of calibration and clinical usefulness remain as important as discrimination, but they are not frequently discussed. Calibration is a model’s reflection of actual outcome prevalence in its predictions. Clinical usefulness refers to the utilities, costs, and harms of using a predictive model in practice. A decision analytic approach to calibrating and selecting an optimal intervention threshold may help maximize the impact of readmission risk and other preventive interventions.
Objectives
To select a pragmatic means of calibrating predictive models that requires a minimum amount of validation data and that performs well in practice. To evaluate the impact of miscalibration on utility and cost via clinical usefulness analyses.
Materials and methods
Observational, retrospective cohort study with electronic health record data from 120,000 inpatient admissions at an urban, academic center in Manhattan. The primary outcome was thirty-day readmission for three causes: all-cause, congestive heart failure, and chronic coronary atherosclerotic disease. Predictive modeling was performed via L1-regularized logistic regression. Calibration methods were compared including Platt Scaling, Logistic Calibration, and Prevalence Adjustment. Performance of predictive modeling and calibration was assessed via discrimination (c-statistic), calibration (Spiegelhalter Z-statistic, Root Mean Square Error [RMSE] of binned predictions, Sanders and Murphy Resolutions of the Brier Score, Calibration Slope and Intercept), and clinical usefulness (utility terms represented as costs). The amount of validation data necessary to apply each calibration algorithm was also assessed.
Results
C-statistics by diagnosis ranged from 0.7 for all-cause readmission to 0.86 (0.78–0.93) for congestive heart failure. Logistic Calibration and Platt Scaling performed best and this difference required analyzing multiple metrics of calibration simultaneously, in particular Calibration Slopes and Intercepts. Clinical usefulness analyses provided optimal risk thresholds, which varied by reason for readmission, outcome prevalence, and calibration algorithm. Utility analyses also suggested maximum tolerable intervention costs, e.g., $1720 for all-cause readmissions based on a published cost of readmission of $11,862.
Conclusions
Choice of calibration method depends on availability of validation data and on performance. Improperly calibrated models may contribute to higher costs of intervention as measured via clinical usefulness. Decision-makers must understand underlying utilities or costs inherent in the use-case at hand to assess usefulness and will obtain the optimal risk threshold to trigger intervention with intervention cost limits as a result.}
}


@article{guyon,
  author = {Isabelle Guyon and Andrè Elisseeff},
  title  = {An Introduction to Variable and Feature Selection},
  journal = {Journal of Machine Learning Research},
  year = {2003},
  volume = {3},
  pages = {1157-1182}
}
