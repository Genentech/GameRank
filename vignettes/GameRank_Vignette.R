#' ---
#' title: "GameRank"
#' author: 
#'   - Carsten Henneges
#'   - Joseph N Paulson
#' output: 
#'   rmarkdown::html_vignette:
#'     toc: true
#'     toc_depth: 2
#' date: "`r format( Sys.time() )`"
#' bibliography: 
#'   - ../ms/game_rank.bib
#'   - game_rank_vignette.bib
#' vignette: >
#'   %\VignetteIndexEntry{GameRank}
#'   %\VignetteEngine{knitr::rmarkdown}
#'   %\VignetteEncoding{UTF-8}
#' ---
# /* Hit Ctrl + Shift + K in Rstudio to generate html_document for quick view. Note: Vignette YAML header doesn't work then. */
# /* Run knitr::spin("vignettes/01_GameRank.R", format="Rmd") to turn this into a Vignette */

#+ setup, eval=TRUE, include=FALSE
rm( list=ls() )
library( dplyr )
# library( tidyr )
library( purrr )
library( ggplot2 )

devtools::load_all("~/GameRank/")
knitr::opts_chunk$set(eval = TRUE, message=FALSE, warning=FALSE) # Control if chunks are run or not

#' # Introduction to GameRank
#' 
#' GameRank is a package for feature selection. With the advent of omics technologies and large
#' multi-variate and multi-level datasets comprising 1,000s of features, feature selection became a necessary and critical
#' part of modern data analysis. Goals are three-fold: improving prediction performance of trained models by
#' removing irrelevant and noisy features (predictive modelling), obtaining sparse and parsymonious models (descriptive modelling), 
#' and - hopefully - gaining more insights on explaining the data generating process (explanatory modelling). [@Shmueli2010; Sauerbrei2020]
#' This package and its vignette consider predominantly the predictive but also the descriptive goal while sparing out
#' the explanatory modelling aspect, which first relies on a domain theory and a theoretical model that can be - at best -
#' informed but not be automatically generated by algorithmic feature selection methods.
#' 
#' This vignette describes how to perform wrapper-based feature selection using the GameRank package for predictive and possibly descriptive
#' modelling. In such a feature selection scenario likely the following individual steps will be performed: [@Shmueli2010]
#' 
#' 1. Feature screening - Evaluating missing data, outliers and how much information each variable contains about the outcome, 
#'    that is evaluating mutual information and correlations with the outcome
#' 2. Feature construction - Try to derive more informative or better correlated features from available variables through
#'    combinations and/or functional transformations
#' 3. Feature selection - Apply wrapper-based feature selection by GameRank to obtain an optimized combination for the
#'    prediction model or description model
#' 4. Model evaluation - Check performance of final model on hold-out data, measure its descriptive qualities on the full dataset
#' 
#' These steps may be followed by exploiting the model or deploying the model for usage. Model exploitation may include
#' finding optima or best points to determine next. Deploying the model may be generating nomograms or uploading to a website.
#' 
#' Within this vignette, we'll demonstrate the package funcationality using the following toy dataset
#' (see code in inst/data-raw/toy_data.R for details; it includes four special variables that were sampled from a univariate
#' Normal distribution and then squared, cubed, exp() and power transformed [`the_squared`,`the_cubed`,`the_exped`,`the_power`], and
#' one variable that is sampled from a multi-modal Gaussian Mixture Distributions with two components [`the_multi`]):
#+ width=40
vars <- grep( "the_|rnd", colnames(toy_data), value=TRUE ) # Define a list of variables for selection
resp <- "resp" # Define the response variable (can be a formula term)
summary( toy_data %>% dplyr::select( all_of( c( resp, vars[1:4] ) ) ) )

#' 
#' The split into training, validation and hold-out test set are done when we actually run
#' the wrapper algorithms.
#' 

#' 
#' # 1. Feature screening
#' 
#' Before building a predictive or descriptive model, it's good practice to describe the features.
#' That normally includes looking at the completeness of variables (missing data) and distributional
#' features (like distribution skew, multi-modality or outliers). Additionally it is of interst
#' how much information the data contains about the outcome? Most successful prediction approaches
#' start with such a step. [@Guyon2003]
#' 
#' GameRank provides a one-stop function for that: `check_variables()`. This
#' function compiles the total number of observations, the percentage of non-missing observations and 
#' categorizes completeness into categories from Perfect (100% complete) to Drop (<70% completeness). 
#' 
#' It calculates the variable entropy and mutual information with respect to the response variable, which can be 
#' a categorical, numeric or surival outcome. To this end, it applies histogram density estimation with binwidth 
#' chosen be leave-one-out cross-validation, and estimates the mutual information from the cross-tabulated counts. 
#' This overview information allows to screen, filter or rank for variables with low entropy (coding for single 
#' infrequent values and low correlated variables with the outcome). It classifies variables by entropy into those
#' that can be dropped (entropy<0.001) and might be kept. 
#' 
#' Finally, it determines, for numeric variables, as to whether there are outliers using the robust outlier test 
#' $Q1/Q3 \pm 1.5 \times IQR$ method together with the range-to-sd-ratio as measure for skew. Using this table, we 
#' can easily filter and screen for useful variables.
#' 
#+ width=40
vck <- check_variables( toy_data, resp, vars )
vck %>% 
  filter( !is_response ) %>% 
  arrange( desc(entropy) ) %>% 
  dplyr::select( variable, p, check_missing, entropy, mutual_information, check_entropy ) %>%
  head

#' Ranking and plotting variables by their entropy and mutual information with respect to the response helps to
#' identify variables that are constant or contain low information 
#+ fig.width=7
vck %>% 
  filter( !is_response ) %>%
  mutate( flg = grepl( "the_", variable )) %>%
  ggplot(aes(x=entropy, y=mutual_information, color=flg ) ) +
  geom_point()


#' 
#' Sometimes variable distributions may be multi-modal, that is they may have more than one mode (point of probability 
#' mass concentration). Multiple modes indicate that the data for a variable may come from multiple data generating processes,
#' that should be adjusted or better stratified for in subsequent analyses. 
#' 
#' GameRank provides a function for this task: `check_multimodality`. This function determines multi-modality for
#' continuous numeric variables as follows: 
#' ```
#' for k from 1 to kmax do
#'   - m <- list()
#'   - for i from 1 to m_fits od
#'       - l <- list()
#'       - fit Gaussian Mixture Model g with k components
#'       - if is_converged(g) then add g to list l fi
#'     od
#' od
#' - Find the minimal AIC for which at least min_fits_converged_models were recorded and
#'   retrieve that model as gbest. In case of ties use the one with minimal k.
#' if 1 < n_components(gbest) then 
#'   - determine cut-points between mixture components via root finding algorithm
#' fi
#' ```
#' Cut-points are determined by a standard root finding algorithm, locating - if possible - the points where the 
#' adjacent component distributions scaled by their priors, are equal.
#' 
#' The chance for detecting multi-modal distributions depends on the available data and hence distributions reported
#' may not be multi-modal or may go undetected. Thus a additional visual review of the identified variable distributions is 
#' a good idea, for which the results from `multi_modal` may be used as a starting point.
mumo <- check_multimodality( dat = toy_data, resp = resp, 
                             vars = vars[1:9], n_comp = 3,
                             m_fits = 25,  min_fits_converged = 20 )

#' The result of `check_modality` consists of two list elements: "data" and "transforms". The first, "data", is a copy of the
#' input data with added feature columns, in case a variable was found to be multi-modal. The "transforms" element is a list
#' that comprises details for multi-modal distributions about the algorithm results, i.e. an AIC table and the optimal flexmix
#' model, for instance.
#' 
#' To compile a handy table out of this list, we are using purrr to extract all non-null "transformed_var" slots as a character
#' list. Those contain the names of the newly generated multi-modal variables.
#' 
mumo_vars <- mumo$transforms %>%
  keep( ~ !is.null( .x$transformed_var ) ) %>% 
  map_chr( "transformed_var" ) 
mumo$data[,mumo_vars]

#' Every entry in the transforms list contains an AIC table with the model selection data
mumo$transforms$the_multi$aic_aggregate

#' The best GMM model from flexmix is saved as well. Using the `parameters()` and `prior()` functions
#' we can obtain the differing component distribution parameters (mean and standard deviation) and the
#' prior values (ie, how likely each mixture is).
#' 
#' With parameters from the flexmix package, we get a table where each column reports the mean and variance estimates
#' of the mixture components. With prior we'll get the mixing probability estimates.
mumo$transforms$the_multi$best_model
flexmix::parameters(mumo$transforms$the_multi$best_model)
flexmix::prior(mumo$transforms$the_multi$best_model)
#' We can obtain the cut-points separating both modes:
pcuts <- mumo$transforms$the_multi$cut_points
pcuts
#' We can now display the multi-modal variable with the function `gplot_mulitmodal_variable()`. It displays a
#' histogram and kernel density estimators overlayed by the best Gaussian Mixture distribution model components, and
#' on the right side an Akaike Information Criterion plot with the line connecting those minimal AICs for which enough
#' models converged:
#+ fig.width=7
gplot_multimodal_variable(  dat = toy_data, resp = resp, vars = vars[1:9], 
                            mumo = mumo, variable = "the_multi" )


#' # 2. Feature construction
#' 
#' Another task in predictive modeling is finding variables that are better when they are transformed. [@Sauerbrei2020] refer
#' to this as function selection, [@Guyon2003] as feature construction. While feature selection is an algorithmic area, 
#' __feature construction__ is less automated and requires domain knowledge. There is no unique recipe for constructing better features
#' than trial and error.[@Guyon2003] Hence analysts usually try a list of standard transforms (e.g., square root, log, some powers) to
#' see if the distribution shapes comes closer to Normal, for instance.[@StdTransforms] But also Principal Component Analysis, Independent
#' Component Analysis, Non-negative Matrix Factorization methods can be regarded as feature construction algorithms. Similarly,
#' separating multiple modes - as done before - is a form of feature construction.
#' 
#' In the remainder of the last section, we observed a multi-modal variable. As cut-points are determined by `check_multimodality` in those 
#' cases, we can easily add a new feature to our dataset that helps to distinguish between the component distributions:
toy_data <- toy_data %>% 
  bind_cols( mumo$data[,mumo$transforms$the_multi$transformed_var] )
vars <- c(vars, mumo$transforms$the_multi$transformed_var )

#' GameRank also provides a handy function to check if standard transforms improve Normality of the features.[@StdTransforms] 
#' The one-stop function `simple_transforms` in GameRank transforms each variable in the list by a square root, cube root, log and z-score 
#' transformation. Those that improve the Shapiro-Wilk W-statistics are then retrained and added to the dataset:
smp <- simple_transforms( toy_data, vars = vars )

#' To see which variables were found, let's shape the output into a tibble:
tfs <- purrr::map_dfr(smp$transformations, ~ . )

#' Let's filter and see which transforms gave the best improvements in Normality:
tfs %>% group_by( variable ) %>% filter( max(W)==W ) %>% head
tfs %>% pull( transform ) %>% table 

#' We can add some further transformed variables to our dataset which we'll next use for 
#' feature selection:
svars <- tfs %>%
  group_by( variable ) %>% 
  filter( max(W)==W ) %>% 
  filter( "identity"!=transform )
toy_data <- toy_data %>% bind_cols( smp$data[,svars$transformed_var] )
vars <- c(vars, svars$transformed_var )

#' Other feature construction approaches are available through GameRank, like Power-Transformations 
#' via the Box-Cox transformation. Example code for those can be found at `box_cox_normal` and `box_cox_binomial`.
#' 

#' 
#' # 3. Feature selection
#' 
#' Feature selection can be done via three algorithmic approaches: [@Guyon2003] 
#' 
#' 1. Filters that estimate and apply statistical quantities, like the correlation or mutual information, to select the best features; 
#' 2. Embedded methods that incorporate feature selection into model inference, like decision trees or gradient boosting methods; and 
#' 3. Wrapper methods that make use of a model training algorithm as a black-box to do a combinatorial search in the feature space, 
#'    evaluating each selected combination via an user-defined loss function.
#' 
#' The third approach allows to directly optimize measures for calibration or discrimination and hence
#' from a predictive modeling perspective seems to be desirable. Thus, let's run two of those feature selection algorithms, the 
#' bidirectional search that applies forward and backward selection and the novel GameRank algorithm.
#' 
#' First, we'll split the dataset into thirds: one for training the model, one for validating it and one final hold-out dataset. We'll generate and
#' shuffle a vector or 1s, 2s and 3s which is as long as we have observations. Then we'll separate rows with 3s (hold-off out).
#' 
rr <- rep_len( c(1L,2L,3L), length.out = nrow(toy_data) ) 
rr <- rr[ order( runif( length(rr) )  )]
df_test <- toy_data[which(3==rr),]              # Hold-out dataset
df_sel  <- toy_data[which(rr %in% c(1L,2L)),]   # Data used for training and validation during wrapper selection

#' Next we'll generate a matrix comprising 1s and 2s for training:validation splits. Each column denotes a separate split and the wrapper
#' algorithm will use those defined splits __in parallel__ for variable selection by averaging the validation performances. The function
#' to do that is `prepare_splits` taking the number `ds` of parallel-splits to generate, the dataset, list of response and features as well as
#' the training and evaluation functions (`fn_train`,`fn_eval`) as arguments. It tries to find splits that are random but include a sufficient number
#' of complete cases for model training. 
ds <- prepare_splits( ds = 3L, dat = df_sel, resp = resp, vars = vars, 
                      fn_train = fn_train_binomial, 
                      fn_eval = fn_eval_binomial_auroc )
ds %>% head(5)

#' 
#' Wrapper selection algorithms are slow combinatorial searches that are not guaranteed to find more than a local optimum. Their performance also
#' depends - in some cases - on the ordering of input variables. Therefore it is a good idea to rerun each algorithm with shuffled orders of features
#' to obtain a set of feature selections to choose from.
#' 
#' Here we'll sort variables by their the mutual information with regards to the response:
vck <- check_variables( df_sel, resp, vars )
vars <- vck %>% 
  filter( !is_response) %>% 
  arrange( desc(mutual_information) ) %>% 
  pull( variable )

#' Let's run the first wrapper: **bidirectional search**, an algorithm that performs a forward and a backward selection step per iteration and
#' ensures that all iterations converge to the same partition by constraining the searched variables for the forward and backward steps.
bds <- bidirectional( dat = df_sel, resp = resp, vars = vars, 
                      fn_train = fn_train_binomial, 
                      fn_eval = fn_eval_binomial_auroc, 
                      m = 6L, ds = ds, maximize = TRUE )
bds$variable_selections
bds$agg_results %>% 
  as.data.frame %>%
  arrange( desc(mean_validation) ) %>% 
  filter( opt ) %>%
  dplyr::select( ch_selection, mean_train, mean_validation, mean_bias ) %>%
  head( 5 ) # Display top 5 optimal selections

#' Let's run GameRank next. GameRank doesn't require the data to be split into training and validation sets. It receives a `dsi` parameter with
#' an index vector of 1s and 2s that is then repeated up to the length of the dataset and thereby defines the relative proportions of training 
#' to validation split per round. In small sample feature selection scenarios `dsi` is set to just 2s such that all data are put into the validation
#' split where the `fn_eval` function performs a bootstrap or cross-validation (see small sample example code for details). 
gmr <- game_rank( dat = df_sel, resp = resp, vars = vars, 
                  fn_train = fn_train_binomial, fn_eval = fn_eval_binomial_auroc, 
                  m = 6L, 
                  # dsi = c(1,2) means 50:50 training:validation split each round; 
                  # c(1,1,2) would be 2/3rds training to 1/3rds validation, and so forth
                  dsi = c(1L,2L),
                  maximize = TRUE, 
                  team_size = 4L, rounds = 10L, min_matches_per_var = 7L )
gmr$variable_ranking %>% 
  head( 10 )
gmr$game_rank_selection

#' 
#' # 4. Model evaluation
#' 
#' Having selected an optimized feature set, the model prediction performance should be measured on the hold-out test dataset. A model is 
#' __calibrated__ if it's predictions match with the obersvations. That can be easily plotted for regression models, but involves estimating
#' the unobserved probability distributions for binary and survival outcomes.
#' 
#' GameRank provides plotting functions for these cases (e.g., `gplot_predictions_binomial` and `gplot_predictions_cox`). These functions 
#' receive a dataset, response and feature lists, and a final model. Usually this finaly model, in this example obtained before from `fn_train_binomial`,
#' is derived from training plus validation data and calibration is plotted on the hold-out test data.
#' 
#' Let plot this for both selections obtained from the bidirectional search and GameRank:
#+ fig.width=7
bds_fsel <- bds %>% 
  purrr::pluck( "variable_selections" ) %>% 
  purrr::pluck( 1L)
mod_bds <- fn_train_binomial( dat = df_sel, resp = resp, 
                              selection = bds_fsel  )
mod_bds
gplot_predictions_binomial( dat = df_test, resp = resp, 
                            selection = bds_fsel, mod = mod_bds )

#+ fig.width=7
gmr_fsel <- gmr %>% purrr::pluck( "game_rank_selection" ) 
mod_gmr <- fn_train_binomial( dat = df_sel, resp = resp, 
                              selection = gmr_fsel  )
mod_gmr
gplot_predictions_binomial( dat = df_test, resp = resp, 
                            selection = bds_fsel, mod = mod_gmr )

#' A last step is to understand the influence of observations on the predictive modeling. The goal is to 
#' identify influential observations that have impacted the model fit. GameRank identifies these influential_observations as
#' observations that, if they are removed, reduce or increase a model fit or any model parameter by more than $Q1 - 1.5 \times IQR$ and $Q3 + 1.5 \times IQR$.
#' 
#' There is again a one-stop function `influential_observations` the calculates those __dffits__- and __dfbeta__-like measures for each observation
#' and flags observations accordingly:#' 
ifo <- influential_observations( df_sel, resp, gmr_fsel, 
                                 fn_train_binomial, fn_eval_binomial_auroc, 
                                 fn_infl_coefficients, fn_predict_glm )
ifo %>% 
  as.data.frame %>% 
  filter( is_influential )  %>%
  dplyr::select( row, is_influential_co, dffit ) %>%
  head( 5 )

#' # References
#' 

